# ── App settings ─────────────────────────────────────────────────────────────
FLASK_PORT=5000
FLASK_DEBUG=false
MAX_UPLOAD_MB=50

# ── LLM provider (optional) ─────────────────────────────────────────────────
# Set LLM_PROVIDER to enable server-side defaults so users don't need to
# configure the LLM panel in the UI every time.
# Supported values: ollama, openai, gemini  (leave empty to disable)
LLM_PROVIDER=
LLM_MODEL=
LLM_API_KEY=
LLM_ENDPOINT=

# Provider-specific examples:
#
# Ollama (local):
#   LLM_PROVIDER=ollama
#   LLM_MODEL=llama3
#   LLM_ENDPOINT=http://localhost:11434
#
# OpenAI:
#   LLM_PROVIDER=openai
#   LLM_MODEL=gpt-4o
#   LLM_API_KEY=sk-...
#
# Gemini:
#   LLM_PROVIDER=gemini
#   LLM_MODEL=gemini-2.0-flash
#   LLM_API_KEY=AI...
